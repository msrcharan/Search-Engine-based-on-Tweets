# -*- coding: utf-8 -*-
"""cosine_similarity.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SUQJL51zNlXPBumrOZrZ4Xn5yYkTbsp4
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install scikit-learn

from sklearn.feature_extraction.text import TfidfVectorizer
import glob
import os

data_dir = '/content/drive/MyDrive/tweet_files'
file_list = glob.glob(os.path.join(data_dir, '*.txt'))

file_subset = file_list[0:1000]

# Read the text from each file and store it in a list
texts = []
for filename in file_subset:
    with open(os.path.join('/content/drive/MyDrive/tweet_files', filename), 'r') as f:
        texts.append(f.read())



#print(file_subset[0:5])
texts[0:2]

vectorizer = TfidfVectorizer()

vectorizer = TfidfVectorizer(stop_words=None)

vectorizer = TfidfVectorizer(min_df=0.005)

vectors = vectorizer.fit_transform(texts)

from sklearn.metrics.pairwise import cosine_similarity

similarities = cosine_similarity(vectors)

print((similarities[26][97]))

from sklearn.feature_extraction.text import CountVectorizer
count_vectorizer =CountVectorizer(stop_words='english', min_df=0.00005) 
corpus2 = count_vectorizer.fit_transform(texts) 
print(count_vectorizer.get_feature_names())

import pandas as pd
df= pd.DataFrame(corpus2.toarray(),columns=count_vectorizer.get_feature_names())
df2 = pd.DataFrame(cosine_similarity(df, dense_output=True))
df2.head()

df2

from sklearn.feature_extraction.text import CountVectorizer
count_vectorizer = CountVectorizer(stop_words='english')
count_vectorizer = CountVectorizer()
sparse_matrix = count_vectorizer.fit_transform(texts)
doc_term_matrix = sparse_matrix.todense()
df = pd.DataFrame(doc_term_matrix,columns=count_vectorizer.get_feature_names())
df.head()

from sklearn.metrics.pairwise import cosine_similarity
dj=pd.DataFrame(cosine_similarity(df, dense_output=True))
dj.head()